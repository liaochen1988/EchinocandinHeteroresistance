{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313f7541-f6b6-4077-b976-9520575aff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edccba2d-fe19-4441-b064-1ea230140314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863aceef-6571-46aa-9196-c8bea924ed94",
   "metadata": {},
   "source": [
    "### Each ORF has two features.\n",
    "### A quanlitative feature describes whether an ORF has 2 copies (normal, 0), or has more than 2 copies over the entire region (full amplification, 1), or has more than 2 copies over part of the region (partial amplification), or has less than 2 copies over the entire region (full deletion, 3), or has less than 2 copies over part of the region (partial deletion, 4).\n",
    "### A quantitative feature describes the quantitative copy number of the region where amplification or deletion occurs. If neither amplification nor deletion is detected, this feature has a value of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9380749-fcbe-4f99-989f-0f589e4ec76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cnv_features(\n",
    "    isolate,   # isolate ID \n",
    "    filepath,  # file path of coverage density histogram\n",
    "    min_percent_peak_distance = 0.9,  # identified peaks must be at least 0.9 copy between each other \n",
    "    min_percent_orf_length = 0.2  # peaks that cover less than 20% of an ORF are discarded\n",
    "):\n",
    "    #-------------------------------------------------------------------\n",
    "    # read histogram file and keep only ORFs (those starting with CPAR2)\n",
    "    #-------------------------------------------------------------------\n",
    "    if filepath.endswith('.gz'):\n",
    "        df_hist = pd.read_csv(filepath, sep=\"\\t\", compression='gzip', header=None, low_memory=False)\n",
    "    else:\n",
    "        df_hist = pd.read_csv(filepath, sep=\"\\t\", header=None, low_memory=False)\n",
    "    df_hist = df_hist[df_hist[3].str.startswith('CPAR2_')]\n",
    "    df_hist = df_hist[df_hist[0] != 'all']\n",
    "\n",
    "    #--------------------------------------\n",
    "    # compute average read coverage per orf\n",
    "    #--------------------------------------\n",
    "    df_hist2 = deepcopy(df_hist)\n",
    "    df_hist2[8] = df_hist2[4]*df_hist2[7]\n",
    "    df_hist2 = df_hist2.groupby([0,1,2,3])[8].agg(np.sum).reset_index().sort_values([0,1,2])\n",
    "    df_hist2.columns = ['Chromosome','StartPos','StopPos','ORF','AverageReads']\n",
    "    \n",
    "    #-------------------------------------------------------\n",
    "    # compute average read coverage per (single-copy) genome\n",
    "    #-------------------------------------------------------\n",
    "    # use orfs with average read counts between 1% and 99% to fit kernel\n",
    "    orf_reads = list(df_hist2.AverageReads)\n",
    "    qlow = np.percentile(orf_reads, 1)\n",
    "    qhigh = np.percentile(orf_reads, 99)\n",
    "    orf_reads = list(df_hist2[(df_hist2.AverageReads<=qhigh) & (df_hist2.AverageReads>=qlow)].AverageReads)\n",
    "\n",
    "    # fit a Gaussian kernel and find peaks\n",
    "    kernel = gaussian_kde(orf_reads)\n",
    "    xs = np.arange(np.min(orf_reads),np.max(orf_reads)+1)\n",
    "    peak_indices, _ = find_peaks(kernel(xs))\n",
    "\n",
    "    # find the peak location with maximum height\n",
    "    max_height_peak_location = None\n",
    "    max_height = -1\n",
    "    for peak_index in peak_indices:\n",
    "        if kernel(xs[peak_index])[0] > max_height:\n",
    "            max_height = kernel(xs[peak_index])\n",
    "            max_height_peak_location = xs[peak_index]\n",
    "\n",
    "    ave_reads_per_genome_copy = max_height_peak_location/2.0\n",
    "    # print(\"Average reads per genome copy = %2.2f.\"%(ave_reads_per_genome_copy))\n",
    "    \n",
    "    #----------------------------------------\n",
    "    # loop over orfs and compute cnv features\n",
    "    #----------------------------------------\n",
    "    df_hist = df_hist[[3,4,5,6]]\n",
    "    df_hist.columns = ['Orf','Depth','Count','Length']\n",
    "    df_hist[['Depth','Count','Length']] = df_hist[['Depth','Count','Length']].astype(int)\n",
    "\n",
    "    res = []\n",
    "    all_orfs = list(set(df_hist.Orf))\n",
    "    for k, orf in enumerate(all_orfs):\n",
    "        df_hist_orf = df_hist[df_hist.Orf==orf].reset_index(drop=True)\n",
    "        min_depth = np.min(df_hist_orf.Depth)\n",
    "        max_depth = np.max(df_hist_orf.Depth)\n",
    "        orf_length = list(df_hist_orf.Length)[0]\n",
    "        \n",
    "        if len(df_hist_orf) == 1:\n",
    "            # if only 1 read depth is available, it must be 0 (a full deletion)\n",
    "            if min_depth>0:\n",
    "                raise Exception(\"Only 1 read depth is available but minumum coverage is not zero.\")\n",
    "            res.append([isolate, orf, 'fulldel', 0]) # full deletion, 0\n",
    "        else:    \n",
    "            # repeat values at the beginning (it will miss a peak at 0 otherwise)\n",
    "            df_hist_orf = pd.concat([pd.DataFrame([[orf,min_depth-1,1,orf_length]], columns=df_hist_orf.columns), df_hist_orf])\n",
    "        \n",
    "            # fit a gaussian kernel\n",
    "            input_data_for_gaussian_kde = []\n",
    "            for x,y in zip(df_hist_orf.Depth, df_hist_orf.Count):\n",
    "                input_data_for_gaussian_kde.extend([x]*int(y))\n",
    "            kernel = gaussian_kde(input_data_for_gaussian_kde)\n",
    "            xs = np.arange(min_depth-1, max_depth+1)     \n",
    "\n",
    "            # compute average reads per copy of this orf\n",
    "            adjustment_factor=1.0\n",
    "            peak_indices, _ = find_peaks(kernel(xs), distance=1e10) # find the peak with maximum height\n",
    "            if len(peak_indices)>0:\n",
    "                if len(peak_indices)>1:\n",
    "                    raise Exception(\"More than 1 peaks were found.\")\n",
    "                dominant_cnv = xs[peak_indices[0]]/ave_reads_per_genome_copy # this number should be an integer multiple of ave_reads_per_genome_copy\n",
    "                if np.round(dominant_cnv) != 0:\n",
    "                    adjustment_factor = dominant_cnv/np.round(dominant_cnv)\n",
    "            ave_reads_per_orf_copy = ave_reads_per_genome_copy*adjustment_factor\n",
    "        \n",
    "            # find all peaks by requiring that the distance between two peaks have to be as wide as X*100% of average reads per orf copy\n",
    "            peak_indices, _ = find_peaks(kernel(xs), distance=ave_reads_per_orf_copy*min_percent_peak_distance)\n",
    "            peak_indices.sort()\n",
    "            \n",
    "            # find number of base pairs covered by each peak\n",
    "            # the left or right boundary of each peak is defined as the middle point between this peak and its left or right peak\n",
    "            basepairs_per_peak = []\n",
    "            for m, peak_index in enumerate(peak_indices):\n",
    "                cnv = xs[peak_index]/ave_reads_per_orf_copy\n",
    "                if m==0:\n",
    "                    left_boundary = 0\n",
    "                else:\n",
    "                    left_boundary = (xs[peak_indices[m]]+xs[peak_indices[m-1]])/2\n",
    "                if m==len(peak_indices)-1:\n",
    "                    right_boundary = 1e10\n",
    "                else:\n",
    "                    right_boundary = (xs[peak_indices[m]]+xs[peak_indices[m+1]])/2\n",
    "                    \n",
    "                # peak contribution equals to the ratio of #basepairs covered by this peak to the length of this orf\n",
    "                peak_contribution = df_hist_orf[(df_hist_orf.Depth>=left_boundary) & (df_hist_orf.Depth<=right_boundary)].Count.sum()/orf_length\n",
    "                basepairs_per_peak.append([cnv, np.round(cnv), peak_contribution])\n",
    "            df_basepairs_per_peak = pd.DataFrame(basepairs_per_peak, columns=['Cnv_float','Cnv_int','Peak_contribution'])\n",
    "            \n",
    "            if len(df_basepairs_per_peak)==0:\n",
    "                raise Exception(\"No peak was found for ORF %s.\"%(orf))\n",
    "            else:\n",
    "                # renomalize peak contribution\n",
    "                df_basepairs_per_peak.Peak_contribution = df_basepairs_per_peak.Peak_contribution/np.sum(df_basepairs_per_peak.Peak_contribution)\n",
    "            \n",
    "            # filter out small peaks (criteria: percent contribution < min_percent_orf_length (by default: 20%)\n",
    "            df_basepairs_per_peak_filtered = deepcopy(df_basepairs_per_peak[df_basepairs_per_peak.Peak_contribution >= min_percent_orf_length])\n",
    "            \n",
    "            # determine CNV feature values\n",
    "            # copy number of an amplified or deleted region is determined by the biggest peak not at 2\n",
    "            if len(df_basepairs_per_peak_filtered)>0:\n",
    "                if 2 in list(df_basepairs_per_peak_filtered.Cnv_int):\n",
    "                    if len(df_basepairs_per_peak_filtered)==1:\n",
    "                        res.append([isolate, orf, 'normal', 2]) # normal, 2\n",
    "                    else:\n",
    "                        if (len(df_basepairs_per_peak_filtered[df_basepairs_per_peak_filtered.Cnv_int<2])>0) & (len(df_basepairs_per_peak_filtered[df_basepairs_per_peak_filtered.Cnv_int>2])>0):\n",
    "                            # amplification, normal, and deletion regions are simultaneously found in this orf\n",
    "                            # if copy numbers are 1,2 and 3, very likely, it is a very wide single peak that centers at copy = 2\n",
    "                            if set(list(df_basepairs_per_peak_filtered.Cnv_int)) == set([1,2,3]):\n",
    "                                res.append([isolate, orf, 'normal', 2]) # normal, 2\n",
    "                            else:\n",
    "                                cn2 = df_basepairs_per_peak_filtered[df_basepairs_per_peak_filtered.Cnv_int!=2].sort_values('Peak_contribution',ascending=False).iloc[0].Cnv_int\n",
    "                                if cn2 < 2:\n",
    "                                    res.append([isolate, orf, 'prtldel', cn2]) # partial deletion, cn2\n",
    "                                else:\n",
    "                                    res.append([isolate, orf,  'prtlamp', cn2]) # partial amplification, cn2\n",
    "                        else:\n",
    "                            cn2 = df_basepairs_per_peak_filtered[df_basepairs_per_peak_filtered.Cnv_int!=2].sort_values('Peak_contribution',ascending=False).iloc[0].Cnv_int\n",
    "                            if cn2 < 2:\n",
    "                                res.append([isolate, orf, 'prtldel', cn2]) # partial deletion, cn2\n",
    "                            else:\n",
    "                                res.append([isolate, orf,  'prtlamp', cn2]) # partial amplification, cn2\n",
    "                else:\n",
    "                    if (len(df_basepairs_per_peak_filtered[df_basepairs_per_peak_filtered.Cnv_int<2])>0) & (len(df_basepairs_per_peak_filtered[df_basepairs_per_peak_filtered.Cnv_int>2])>0):\n",
    "                        # amplification and deletion regions found in this orf (no 2-copy regions)\n",
    "                        # still, it may be a wide peak that centers at copy = 2\n",
    "                        if set(list(df_basepairs_per_peak_filtered.Cnv_int)) == set([1,3]):\n",
    "                            res.append([isolate, orf, 'normal', 2]) # normal, 2\n",
    "                        else:\n",
    "                            cn2 = df_basepairs_per_peak_filtered.sort_values('Peak_contribution',ascending=False).iloc[0].Cnv_int\n",
    "                            if cn2 < 2:\n",
    "                                res.append([isolate, orf, 'prtldel', cn2]) # partial deletion, cn2\n",
    "                            else:\n",
    "                                res.append([isolate, orf,  'prtlamp', cn2]) # partial amplification, cn2\n",
    "                    else:\n",
    "                        cn2 = df_basepairs_per_peak_filtered.sort_values('Peak_contribution',ascending=False).iloc[0].Cnv_int\n",
    "                        if cn2 < 2:\n",
    "                            res.append([isolate, orf, 'fulldel', cn2]) # full deletion, cn2\n",
    "                        else:\n",
    "                            res.append([isolate, orf,  'fullamp', cn2]) # full amplification, cn2\n",
    "            else:\n",
    "                # none of the peaks covers more than 20% of the orf length\n",
    "                # it may occur when the copy number is very high and multiple peaks may exist (they should belong to the same peak but coverage for each depth is low)\n",
    "                # therfore, we will consider it as a full amplification: keep copy number peaks that are larger than 2 and compute the copy number average\n",
    "                if (len(df_basepairs_per_peak[df_basepairs_per_peak.Cnv_int<2])>0) & (len(df_basepairs_per_peak[df_basepairs_per_peak.Cnv_int>2])>0):\n",
    "                    res.append([isolate, orf, 'normal', 2]) # normal, 2\n",
    "                else:\n",
    "                    df_basepairs_per_peak_filtered = deepcopy(df_basepairs_per_peak[df_basepairs_per_peak.Cnv_int>2])\n",
    "                    if len(df_basepairs_per_peak_filtered)==0:\n",
    "                        raise Exception(\"Weak peaks for ORF %s are not caused by high copy number.\"%(orf))\n",
    "                    else:\n",
    "                        df_basepairs_per_peak_filtered.Peak_contribution = df_basepairs_per_peak_filtered.Peak_contribution/np.sum(df_basepairs_per_peak_filtered.Peak_contribution)\n",
    "                    cn2 = np.round(np.average(df_basepairs_per_peak_filtered.Cnv_float, weights=df_basepairs_per_peak_filtered.Peak_contribution))\n",
    "                    res.append([isolate, orf, 'fullamp', cn2]) # full amplification, cn2\n",
    "                \n",
    "    df_res = pd.DataFrame(res, columns=['Isolate','Orf','CNV_cat','CNV_quant'])                \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d4ef9-ef6b-4516-be04-578fe4dd724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv_feature_array = Parallel(n_jobs=-1)(delayed(compute_cnv_features)(isolate, 'data/%s.read_density.hist.txt.gz'%(isolate)) for isolate in ['E66','MSK2384','CDC340','UWM1206','GL62'])\n",
    "df_cnv_features = pd.concat(cnv_feature_array)\n",
    "df_cnv_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83376a1d-0ff3-4c11-9578-56d7a8d96473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnv_features.to_csv(\"output/cnv_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49e25e-ac2c-490a-8e43-846fbf1c34a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
